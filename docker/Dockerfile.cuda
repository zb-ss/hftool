# hftool Docker image for NVIDIA CUDA
# Based on official PyTorch CUDA image
#
# Build: docker build -f docker/Dockerfile.cuda -t hftool:cuda .
# Run:   docker run --gpus all -it hftool:cuda

ARG CUDA_VERSION=12.4
ARG PYTORCH_VERSION=2.5.1

FROM pytorch/pytorch:${PYTORCH_VERSION}-cuda${CUDA_VERSION}-cudnn9-runtime

LABEL maintainer="zashboy <info@zashboy.com>"
LABEL description="hftool - HuggingFace CLI for NVIDIA CUDA GPUs"
LABEL cuda.version="${CUDA_VERSION}"

# Prevent interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    # FFmpeg for video/audio processing
    ffmpeg \
    # Audio libraries
    libsndfile1 \
    libsox-dev \
    # Image libraries
    libjpeg-dev \
    libpng-dev \
    # Build tools (for some pip packages)
    build-essential \
    # Git (for installing from git repos if needed)
    git \
    # Useful utilities
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Set up Python environment
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Upgrade pip and install build dependencies
# setuptools-scm must be installed BEFORE building hftool
RUN pip install --no-cache-dir --upgrade pip setuptools wheel setuptools-scm

# Install hftool with all dependencies
COPY pyproject.toml README.md ./
COPY hftool/ ./hftool/

# Set version for setuptools-scm (no .git in Docker context)
ARG HFTOOL_VERSION=0.5.0
ENV SETUPTOOLS_SCM_PRETEND_VERSION=${HFTOOL_VERSION}
ENV SETUPTOOLS_SCM_PRETEND_VERSION_FOR_HFTOOL=${HFTOOL_VERSION}

# Write version file directly to bypass setuptools-scm entirely
RUN echo "__version__ = '${HFTOOL_VERSION}'" > hftool/_version.py

# Install hftool with all optional dependencies
# --no-build-isolation ensures env vars are passed to the build
RUN pip install --no-cache-dir --no-build-isolation ".[all]"

# Pre-download common model components to speed up first run
RUN python -c "from transformers import AutoTokenizer; print('Transformers ready')"
RUN python -c "from diffusers import DiffusionPipeline; print('Diffusers ready')"

# Create directories for model cache and config
RUN mkdir -p /root/.cache/huggingface /root/.hftool

# Set default environment variables
ENV HFTOOL_MODELS_DIR=/root/.cache/huggingface
ENV HF_HOME=/root/.cache/huggingface
ENV TRANSFORMERS_CACHE=/root/.cache/huggingface

# Verify CUDA is working
RUN python -c "import torch; print(f'PyTorch {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device count: {torch.cuda.device_count()}')" || echo "GPU check will run at runtime"

# Set working directory
WORKDIR /workspace

# Default command - show help
CMD ["hftool", "--help"]
