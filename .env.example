# =============================================================================
# hftool Environment Variables
# =============================================================================
# Copy this file to .env and customize as needed.
# All variables are optional - defaults work for most setups.
#
# For full documentation, see: docs/environment.md
# =============================================================================

# -----------------------------------------------------------------------------
# Core Settings
# -----------------------------------------------------------------------------

# Directory where models are downloaded and cached
# Default: ~/.cache/huggingface (shared with other HF tools)
#HFTOOL_MODELS_DIR=~/.cache/huggingface

# Path to hftool config directory
# Default: ~/.hftool
#HFTOOL_CONFIG=~/.hftool

# Auto-download models without confirmation prompts
# Values: 1, true, yes (enable) | 0, false, no (disable, default)
#HFTOOL_AUTO_DOWNLOAD=0

# Quiet mode - suppress informational output (errors still shown)
# Values: 1, true, yes (enable) | 0, false, no (disable, default)
#HFTOOL_QUIET=0

# Enable debug mode - show all warnings and verbose output
# Values: 1, true, yes (enable) | 0, false, no (disable, default)
#HFTOOL_DEBUG=0

# Log to file instead of/in addition to console
# Default: empty (no file logging)
#HFTOOL_LOG_FILE=~/.hftool/hftool.log

# Auto-open generated files after creation (images, videos, audio)
# Values: 1, true, yes (enable) | 0, false, no (disable, default)
#HFTOOL_AUTO_OPEN=0

# Start in interactive mode by default
# Values: 1, true, yes (enable) | 0, false, no (disable, default)
#HFTOOL_INTERACTIVE=0

# -----------------------------------------------------------------------------
# GPU Configuration
# -----------------------------------------------------------------------------

# GPU selection: auto, all, or specific GPU indices (0, 1, 0,1, etc.)
# Default: auto (selects best GPU, avoiding display GPU)
#HFTOOL_GPU=auto

# Enable multi-GPU model distribution for large models
# Values: 1, true, yes, balanced (enable) | 0, false, no (disable, default)
# Note: --gpu all sets this automatically
#HFTOOL_MULTI_GPU=0

# CPU offload level for memory-constrained systems
# Values: 0 (disabled, default) | 1 (model offload) | 2 (sequential offload)
#HFTOOL_CPU_OFFLOAD=0

# -----------------------------------------------------------------------------
# AMD ROCm Settings (for AMD GPUs)
# -----------------------------------------------------------------------------

# Custom ROCm installation path (e.g., Ollama's bundled ROCm)
# Default: system ROCm (/opt/rocm)
#HFTOOL_ROCM_PATH=/opt/rocm

# Override GPU architecture (for unsupported GPUs)
# Example: gfx1030 for RX 6000 series, gfx1100 for RX 7900 XTX
# See: https://rocm.docs.amd.com/
#HSA_OVERRIDE_GFX_VERSION=

# Select which AMD GPUs are visible (comma-separated indices)
# Example: 0,1 to use first two GPUs
#HIP_VISIBLE_DEVICES=
#ROCR_VISIBLE_DEVICES=

# Suppress MIOpen (ROCm deep learning library) log messages
# Values: 1 (errors only), 2 (warnings), 3 (info), 4 (silent)
#MIOPEN_LOG_LEVEL=4

# PyTorch memory allocator configuration for ROCm
# Default: expandable_segments:True (reduces fragmentation)
#PYTORCH_ALLOC_CONF=expandable_segments:True

# Enable experimental AOTriton optimizations for RDNA3
# Default: 1 (enabled for RDNA3 GPUs)
#TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1

# Use hipBLAS instead of hipBLASLt (better for consumer GPUs)
# Default: 0 (use hipBLAS)
#TORCH_BLAS_PREFER_HIPBLASLT=0

# -----------------------------------------------------------------------------
# NVIDIA CUDA Settings (for NVIDIA GPUs)
# -----------------------------------------------------------------------------

# Select which NVIDIA GPUs are visible (comma-separated indices)
# Example: 0,1 to use first two GPUs
#CUDA_VISIBLE_DEVICES=

# PyTorch CUDA memory allocator configuration
# Note: PYTORCH_CUDA_ALLOC_CONF is deprecated, use PYTORCH_ALLOC_CONF
#PYTORCH_ALLOC_CONF=

# -----------------------------------------------------------------------------
# HuggingFace Settings
# -----------------------------------------------------------------------------

# HuggingFace API token (required for gated models like FLUX, Llama, etc.)
# Get your token at: https://huggingface.co/settings/tokens
#HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Alternative: HUGGINGFACE_TOKEN (both work)
#HUGGINGFACE_TOKEN=

# HuggingFace cache directory (shared by all HF libraries)
# Default: ~/.cache/huggingface
#HF_HOME=~/.cache/huggingface

# Enable fast downloads with hf_transfer
# Requires: pip install hf_transfer
# Values: 1 (enable) | 0 (disable, default)
#HF_HUB_ENABLE_HF_TRANSFER=0

# Disable download progress bars (useful for CI/CD)
# Values: 1 (disable) | 0 (show, default)
#HF_HUB_DISABLE_PROGRESS_BARS=0

# -----------------------------------------------------------------------------
# Docker Settings (INTERNAL - set automatically, do not modify)
# -----------------------------------------------------------------------------
# These variables are set automatically by `hftool docker run` to enable
# path translation between host and container. You should NOT set these manually.
#
# HFTOOL_IN_DOCKER=1        # Signals we're inside a container
# HFTOOL_HOST_HOME=/home/host  # Mount point for host home directory
# HFTOOL_REAL_HOME=/home/user  # Actual home path on the host system
#
# When HFTOOL_IN_DOCKER=1, paths like ~/Videos/cat.mp4 are automatically
# translated to container paths like /output/cat.mp4

# -----------------------------------------------------------------------------
# Task-Specific Settings
# -----------------------------------------------------------------------------

# GLM-TTS model path (for Chinese TTS)
#GLMTTS_PATH=./GLM-TTS
