[build-system]
requires = ["setuptools>=61.0", "wheel", "setuptools-scm>=8.0"]
build-backend = "setuptools.build_meta"

[project]
name = "hftool"
dynamic = ["version"]
description = "A CLI for Hugging Face models: text-to-image, text-to-video, TTS, ASR, and more."
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT License"}
authors = [
  {name = "zashboy", email = "zashboy@gmail.com"},
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Operating System :: OS Independent",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Utilities",
    "Environment :: Console",
    "Typing :: Typed",
]

dependencies = [
    "transformers>=4.45.0",
    # NOTE: torch is NOT included here - users must install the correct version for their platform:
    #   NVIDIA: pip install torch torchvision torchaudio
    #   AMD ROCm: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2
    #   CPU only: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
    "click>=8.0.0",
    "huggingface_hub>=0.20.0",
    "rich>=13.0.0",
    "python-dotenv>=1.0.0",
]

[project.optional-dependencies]
# Image handling (for image inputs/outputs)
with_image = ["Pillow>=9.0.0"]

# Audio handling (for TTS/ASR)
with_audio = ["soundfile>=0.12.0"]

# HTTP support (for URL inputs)
with_http = ["requests>=2.20.0"]

# Text-to-Image generation (Z-Image, SDXL, FLUX)
# Image-to-Image generation (Qwen Image Edit, SDXL)
# Note: Qwen Image Edit requires diffusers>=0.36.0 for QwenImageEditPlusPipeline
with_t2i = [
    "diffusers>=0.36.0",
    "Pillow>=9.0.0",
    "accelerate>=0.26.0",
    "protobuf>=3.20.0",
    "sentencepiece>=0.1.99",
]

# Text-to-Video generation (HunyuanVideo, CogVideoX, Wan2.2)
# Requires system ffmpeg for video encoding
with_t2v = [
    "diffusers>=0.36.0",
    "Pillow>=9.0.0",
    "accelerate>=0.26.0",
]

# Text-to-Speech (Bark, MMS-TTS)
# Requires system ffmpeg for MP3 conversion
with_tts = [
    "transformers>=4.45.0",
    "soundfile>=0.12.0",
]

# Speech-to-Text / ASR (Whisper)
with_stt = [
    "transformers>=4.45.0",
    "soundfile>=0.12.0",
]

# Advanced audio processing (resampling, feature extraction)
with_audio_advanced = [
    "soundfile>=0.12.0",
    "librosa>=0.9.0",
    "scipy",
]

# Advanced vision processing
with_vision_advanced = [
    "Pillow>=9.0.0",
    "opencv-python-headless>=4.5.0",
]

# Model quantization (requires CUDA/ROCm)
with_quantization = ["bitsandbytes>=0.39.0"]

# Development dependencies
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
]

# All dependencies (for full functionality)
all = [
    # Image
    "Pillow>=9.0.0",
    # Audio
    "soundfile>=0.12.0",
    # HTTP
    "requests>=2.20.0",
    # Diffusers (T2I, I2I, T2V) - 0.36.0+ for QwenImageEditPlusPipeline
    "diffusers>=0.36.0",
    "accelerate>=0.26.0",
    # Tokenizers (protobuf for T5, sentencepiece for various)
    "protobuf>=3.20.0",
    "sentencepiece>=0.1.99",
    # Audio advanced
    "librosa>=0.9.0",
    "scipy",
    # Vision advanced
    "opencv-python-headless>=4.5.0",
]

[project.scripts]
hftool = "hftool.cli:main"

[project.urls]
Homepage = "https://github.com/zb-ss/hftool"
Repository = "https://github.com/zb-ss/hftool"
Issues = "https://github.com/zb-ss/hftool/issues"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --tb=short"

[tool.setuptools.packages.find]
where = ["."]
include = ["hftool*"]

[tool.setuptools_scm]
# Version from git tags (e.g., v0.2.1 -> 0.2.1)
version_scheme = "guess-next-dev"
local_scheme = "no-local-version"
